{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataParallel\n",
    "引入 PyTorch 模块和定义参数。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 并行处理示例\n",
    "### 常用参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5\n",
    "output_size = 2\n",
    "batch_size = 30\n",
    "data_size = 100\n",
    "# 设备\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 随机生成数据集\n",
    "随机生成一个数据集集，并且实现访问元素和返回长度两个方法。创建data_loader。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset): # 继承Dataset\n",
    "    def __init__(self,size,length):\n",
    "        self.len = length\n",
    "        self.data = torch.randn(length,size)\n",
    "    \n",
    "    def __getitem__(self,index): # 访问元素\n",
    "        return self.data[index]\n",
    "    \n",
    "    def __len__(self): # 返回长度\n",
    "        return self.len\n",
    "# 创建数据集加载器，维度为(input_size,data_size), 一次读入块大小为30，开启随机打乱\n",
    "dataset=RandomDataset(input_size,data_size)\n",
    "rand_loader = DataLoader(dataset=dataset,batch_size=batch_size,shuffle=True) # shuffle开启随机打乱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简单模型\n",
    "为了做一个小demo，我们的模型只是获得了一个输入，执行一个线性操作，然后给一个输出。尽管如果，你可使用DataParallel 在任何模型（CNN，RNN，Capsule Net等）。  \n",
    "\n",
    "我们放置了一个输出生命在模型中来检测输出和输入张量大小。请注意在batch rank0中的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple model\n",
    "class Net(nn.Module):\n",
    "    def __init__(self,input_size,output_size):\n",
    "        super(Net,self).__init__()\n",
    "        # simple linear layer\n",
    "        self.fc = nn.Linear(input_size,output_size)\n",
    "    \n",
    "    def forward(self,input):\n",
    "        output = self.fc(input)\n",
    "        print('Model: input size - {input_size}, output size - {output_size}'.format(input_size=input_size,output_size=output_size))\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 使用并行处理\n",
    "**重点**：首先我们需要一个模型的实例，然后验证我们是否拥有多个GPU，如果我们有多个GPU，我们可用nn.DataParallel来包裹我们的模型。然后我们使用model.to(device)把模型放到多个GPU上。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (fc): Linear(in_features=5, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Net(input_size,output_size)\n",
    "# 验证是否有多个GPU\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print('We have {num} GPUs.'.formatm(num=torch.cuda.device_count()))\n",
    "    # 假设有3个GPU，使用并行处理数据的话，就会将维度为[30,x,m,n]的数据分成三个[10,x,m,n],[10,x,m,n],[10,x,m,n]并分别在3个GPU上进行处理数据。\n",
    "    model = nn.DataParallel(model) # 数据划分并移交给GPUs处理\n",
    "model.to(device) # 将模型移交给GPUs处理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 运行模型\n",
    "查看结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 896.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: input size - 5, output size - 2\n",
      "Model: input size - 5, output size - 2\n",
      "Model: input size - 5, output size - 2\n",
      "Model: input size - 5, output size - 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for _, data in enumerate(tqdm(rand_loader)):\n",
    "    input = data.to(device)\n",
    "    output = model(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 结果示例\n",
    "由于我电脑没有GPU，因此我只能给出一些示例图片了。  \n",
    "#### 0/1GPU\n",
    "你只有一个GPU或没有时，当我们获取30个输入和30个输出，模型将期望获得30个输入和30个输出。\n",
    "\n",
    "![figure.1](https://gitee.com/zyp521/upload_image/raw/master/b3kET1.png)\n",
    "\n",
    "#### 多个CPUs\n",
    "如果你有多个GPUs时，你将获得如下的结果：  \n",
    "两个GPUs：\n",
    "\n",
    "![figure.2](https://gitee.com/zyp521/upload_image/raw/master/dZiwXd.png)\n",
    "\n",
    "三个GPUs：\n",
    "\n",
    "![figure.3](https://gitee.com/zyp521/upload_image/raw/master/4E4lje.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 总结\n",
    "数据并行自动拆分了你的数据并且将任务发送到多个GPUs上，当每一个模型完成自己的任务之后，DataParallel收集并且合并这些数据，然后返还给你。（能够提高训练速度）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-use",
   "language": "python",
   "name": "pytorch-use"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
