{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae0bef82-4933-4775-a3c1-53ad19dd54d7",
   "metadata": {},
   "source": [
    "# DataParallel 进行多GPU训练\n",
    "下面将学习如何用DataParalled来使用多GPU，通过PyTorch使用多个GPU非常简单。如下:你可以先将一个模型放在一个GPU下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8490836-8076-41f8-b466-cc176a25264f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "# model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4579d53f-5346-44e3-96bb-0f655ffda993",
   "metadata": {},
   "source": [
    "然后你可以复制所有的张量到GPU:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c78a539a-6b8e-40fe-a4b9-f0cd2dddc37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# mytensor = my_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a8dfe6-777d-4423-8730-d72d8ab98f1a",
   "metadata": {},
   "source": [
    "请注意，只是调用my_tensor.to(device)返回一个my_tensor新的复制在GPU上，而不是重写my_tensor。你需要给他分配一个新的张量并且在GPU上使用这个张量.  \n",
    "\n",
    "在多个GPu中执行前馈，后馈操作是非常自然的，尽管如此，PyTorch默认只会使用一个GPu，通过使用DataParallel让你的模型能够并行运行，你可以很容易在多个GPU上运行你的操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "608209aa-56dd-4149-828c-22f7ceebc431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8e6754-37b6-4ec3-b211-f20e2d3e5c1e",
   "metadata": {},
   "source": [
    "## 引入PyTorch模块和定义参数\n",
    "### 引入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dbe655bd-58f1-4155-b30f-643bd3021814",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader # 引入数据集和数据加载器"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65237c1-df70-428a-ba1b-bb2bf3e10724",
   "metadata": {},
   "source": [
    "### 参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10607f1a-483f-4783-81cc-58d1a013df01",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 5 # 输入channel数\n",
    "output_size = 2 # 输出channel数\n",
    "batch_size = 30 # batch大小\n",
    "data_size = 100 # 数据大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02fe38-13f5-45aa-aa99-83f9226e0fc3",
   "metadata": {},
   "source": [
    "### 设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "398928c8-e10a-4928-ab69-26e386d795fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e38cea-1f9a-47b3-95fd-e5d09f150560",
   "metadata": {},
   "source": [
    "## 简单模型\n",
    "### 实验数据\n",
    "生成数据，实现一个数据类型，然后使用DataLoader进行加载:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0430dc2-e2d4-4cc8-9c1c-fd1a8640dfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomDataset(Dataset):\n",
    "    # fixme: 初始化\n",
    "    def __init__(self, size, length):\n",
    "        self.len = length\n",
    "        self.data = torch.rand(length, size) # 随机生成维度为length×size的数据\n",
    "    \n",
    "    # fixeme: 实现getitem函数，可以用来逐元素获取数据，这个接口应该是要被DataLoader所调用的\n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    # fixme: 返回长度\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7419ff08-b78c-40a5-9a82-fff27a3b2fcb",
   "metadata": {},
   "source": [
    "使用DataLoader进行加载数据:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c83ece7-5be1-40cc-b093-efa3cc49d01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle设置随机抽取，batch_size设置load的batch大小\n",
    "data_loader = DataLoader(dataset=RandomDataset(input_size,data_size), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6619c55e-2381-4b4d-9738-617dfa63e82d",
   "metadata": {},
   "source": [
    "### 实验设计\n",
    "这里仅仅是使用了一个简单的Demo，用来演示可以在多个GPU上并行处理数据，这里使用的模型只是获得一个输入，执行一个线性操作，然后给出一个输出，但是你同样可以使用DataParallel在任何模型上进行使用。  \n",
    "\n",
    "#### 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2f0b9d9d-6f0d-44b3-8939-6513ea787e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc = nn.Linear(input_size, output_size)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        output = self.fc(input)\n",
    "        print(\"模型输入大小:\",input.size(),\"输出大小\",output.size())\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58126100-b7f3-404b-a0d8-5883d33df0ba",
   "metadata": {},
   "source": [
    "#### 创建模型并且进行数据并行处理\n",
    "首先需要一个模型的实例，然后验证我们是否有多个GPU，如果我们有多个GPU，我们就可以用nn.DataParallel来包裹我们的模型。然后我们使用mode.to(device)把模型放到GPU中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a3acc738-1144-41fa-a245-d44122aed999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "目前有 4 个GPUs可以使用.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): Model(\n",
       "    (fc): Linear(in_features=5, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model(input_size, output_size)\n",
    "if torch.cuda.device_count() > 1:\n",
    "    print(\"目前有\",torch.cuda.device_count(),\"个GPUs可以使用.\")\n",
    "    model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9524e35a-3636-4d6d-85db-8ace16c993ce",
   "metadata": {},
   "source": [
    "根据上面的结果可以得到目前你可以使用的GPU给树，并且如果你的GPU个数大于1的时候，就可以将模型使用DataParallel进行包裹，然后再将模型交给设备了.  \n",
    "运行模型，并且要注意将数据也交给device，然后在对数据进行处理."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe5be4d0-c0c5-4445-86b2-523c940eab07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型输入大小: torch.Size([8, 5]) 输出大小 torch.Size([8, 2])\n",
      "模型输入大小: torch.Size([8, 5]) 输出大小 torch.Size([8, 2])\n",
      "模型输入大小: torch.Size([8, 5]) 输出大小 模型输入大小:torch.Size([8, 2])\n",
      " torch.Size([6, 5]) 输出大小 torch.Size([6, 2])\n",
      "步数 1 ——输入的维度: torch.Size([30, 5]) ,输出的维度: torch.Size([30, 2])\n",
      "模型输入大小:模型输入大小: torch.Size([8, 5]) 输出大小 torch.Size([8, 2])\n",
      "模型输入大小: torch.Size([8, 5]) 输出大小 torch.Size([8, 2])\n",
      "模型输入大小: torch.Size([6, 5]) 输出大小 torch.Size([6, 2])\n",
      " torch.Size([8, 5]) 输出大小 torch.Size([8, 2])\n",
      "步数 2 ——输入的维度: torch.Size([30, 5]) ,输出的维度: torch.Size([30, 2])\n",
      "模型输入大小: torch.Size([8, 5]) 输出大小 torch.Size([8, 2])\n",
      "模型输入大小:模型输入大小: torch.Size([8, 5]) 输出大小 torch.Size([8, 2])\n",
      " torch.Size([8, 5]) 输出大小 torch.Size([8, 2])\n",
      "模型输入大小: torch.Size([6, 5]) 输出大小 torch.Size([6, 2])\n",
      "步数 3 ——输入的维度: torch.Size([30, 5]) ,输出的维度: torch.Size([30, 2])\n",
      "模型输入大小: torch.Size([3, 5]) 输出大小 torch.Size([3, 2])\n",
      "模型输入大小: torch.Size([3, 5]) 输出大小 torch.Size([3, 2])\n",
      "模型输入大小: torch.Size([3, 5]) 输出大小 torch.Size([3, 2])\n",
      "模型输入大小: torch.Size([1, 5]) 输出大小 torch.Size([1, 2])\n",
      "步数 4 ——输入的维度: torch.Size([10, 5]) ,输出的维度: torch.Size([10, 2])\n"
     ]
    }
   ],
   "source": [
    "step = 1\n",
    "for data in data_loader:\n",
    "    input = data.to(device)\n",
    "    output = model(input)\n",
    "    print(\"步数\",step,\"——输入的维度:\",input.size(), \",输出的维度:\",output.size())\n",
    "    step += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa72c4-9b73-464c-91c3-bc7cb93a1f1b",
   "metadata": {},
   "source": [
    "这里由于我有4个GPU，因此输出的结果，每一个步数内都有四次model的计算。\n",
    "## 总结\n",
    "数据并行自动拆分了你的数据并且将任务单发送到多个GPU上，当每一个模型都完成自己的任务之后，DataParallel会收集和合并这些结果，然后在返回给你。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
