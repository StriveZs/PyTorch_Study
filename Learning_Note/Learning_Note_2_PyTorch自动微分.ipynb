{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb9cdef-1110-4057-97d1-4f995feaf27c",
   "metadata": {},
   "source": [
    "# PyTorch自动微分\n",
    "autograd包是pytorch中所有神经网络的核心，它为tensors上的所有操作提供自动微分。它是一个由运行定义的框架，这意味着以代码运行方式定义你的后向传播，每次迭代都可以不用(动态的，和tensorflow的静态不同)\n",
    "\n",
    "## Tensor\n",
    "torch.Tensor是包的核心类。如果将其属性.requires_grad设置为True, 则会开始跟踪对tensor的所有操作。完成计算后，可以使用.backward()来自动计算所有梯度。该张量的所有梯度将会积累到.grad属性中。  \n",
    "\n",
    "如果需要停止对tensor历史记录的追踪，则可以调用.detach()，它将其与计算历史记录分离，并防止将来的计算被追踪。  \n",
    "同样如果想要停止追踪历史记录和使用了内存，还可以将代码块使用 with torch.no_grad(): 包装起来。这种操作在对模型进行评估时，十分有用，因为在训练阶段，我们需要将tensor.requires_grad = True，这样方便对训练参数进行调参，而在评估阶段我们不需要进行调参，因此则不需要梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "862c319b-24a6-4956-908f-a9c6c21d1314",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "with torch.no_grad():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e39672-1985-4dc8-97ea-56d0f6c79bf7",
   "metadata": {},
   "source": [
    "还有一个类对autograd实现非常重要，那就是Function。Tensor和Function相互连接并构建一个非循环图，他保存整个完整的计算过程的历史信息。每个张量都有一个.grad_fn属性保存着创建了张量的Function的引用，ps：如果是用户自己创建的张量，则grad_fn是None。  \n",
    "如果你想要计算导数，你可调用Tensor.backward(). 如果Tensor是标量，则不需要指定任何backward(), 如果它有更多的元素，则需要制定一个gradient参数来指定张量的形状。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3757eaf2-adbc-4c77-bdd8-5f150dab0661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 1.],\n",
      "        [1., 1.]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,2, requires_grad=True) # 设置requires_grad = True 来追踪对tensor x的操作\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2322b280-be6a-43c2-ab22-30d1ddd8fd2d",
   "metadata": {},
   "source": [
    "针对上述张量进行操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f38e78da-4b88-4066-aa3c-4aebd66288d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3.],\n",
      "        [3., 3.]], grad_fn=<AddBackward0>)\n",
      "<AddBackward0 object at 0x7f1b2cfd3470>\n"
     ]
    }
   ],
   "source": [
    "y = x + 2\n",
    "print(y)\n",
    "print(y.grad_fn) # 显示y的grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f84bc2-41cf-4f13-b9d1-b70e6ec8d95f",
   "metadata": {},
   "source": [
    "针对y进行更多的操作："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c79fda71-99da-41ae-ad2a-6009563c12ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[27., 27.],\n",
      "        [27., 27.]], grad_fn=<MulBackward0>) tensor(27., grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "print(z,out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f882740-e53b-4a09-98d2-4738070156c8",
   "metadata": {},
   "source": [
    ".requires_grad_()会改变张量的requires_grad标记。输入的标记默认为False。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95e2e635-42a4-44d8-a694-b775e91a3647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "True\n",
      "<SumBackward0 object at 0x7f1ba0354160>\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,2)\n",
    "a = ((a * 3) / (a - 1))\n",
    "print(a.requires_grad)\n",
    "a.requires_grad_(True)\n",
    "print(a.requires_grad)\n",
    "b = (a * a).sum()\n",
    "print(b.requires_grad)\n",
    "print(b.grad_fn)\n",
    "c = torch.rand(2,3)\n",
    "print(c.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc6446a-ac4b-4add-8400-ceab257c9573",
   "metadata": {},
   "source": [
    "从上面例子可以看出，在没有对requires_grad进行赋值的之前默认是False，在使用requires_grad_()进行赋值之后，变为了True，同样如果将a的计算结果赋给b之后，可以看到b的requires_grad也变成了True，而c和a、b没有任何关系，因此c的requires_grad还是False。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07acfdb8-7221-4428-8782-87d6a5aad16e",
   "metadata": {},
   "source": [
    "### 梯度\n",
    "现在实现对out的向后传播，并打印出来x的梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ff90341e-e42b-4310-88d7-8ee8040251b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5000, 4.5000],\n",
      "        [4.5000, 4.5000]])\n"
     ]
    }
   ],
   "source": [
    "out.backward()\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aec839c-aff2-4b27-93a7-507b6c6d0b8f",
   "metadata": {},
   "source": [
    "这里设计的原理就是模式识别中讲到的反向传播的递推梯度计算公式了，如果不知道的可以自行去查阅了。  \n",
    "\n",
    "下面看一个雅克比行列式向量积的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "17ef19c2-9d10-4d71-960c-d252e1c8af08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.9500,  0.0213,  2.2834], grad_fn=<MulBackward0>)\n",
      "tensor([-0.9500,  0.0213,  2.2834])\n",
      "tensor([-486.3931,   10.9079, 1169.0762], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3,requires_grad=True)\n",
    "y = x * 2\n",
    "print(y)\n",
    "print(y.data) # 读取tensor的数值用的\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eab8f69-a2de-4f0e-8f6b-ce5108155190",
   "metadata": {},
   "source": [
    "在这种情况下，y不再是一个标量，因为对y的计算操作会导致y进行变换。torch.autograd不能直接计算整个雅格比，但是如果我们只想要雅格比向量积，只需要简单的传递向量给backward作为参数即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7f2e26af-2d06-4afc-bd77-16dc4a8fae79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.0240e+02, 1.0240e+03, 1.0240e-01])\n"
     ]
    }
   ],
   "source": [
    "v = torch.tensor([0.1,1.0,0.0001],dtype = torch.float)\n",
    "y.backward(v)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "942daa8e-1422-466d-b04e-20a97d629841",
   "metadata": {},
   "source": [
    "你也可将代码放在with torch.no_grad()中来停止对历史的跟踪中的.requires_grad=True的张量自动求导。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "47b54a04-71f6-4753-9f2e-1cd940110621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "print(x.requires_grad)\n",
    "print((x ** 2).requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    print((x ** 2).requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5aa9749-d293-48d0-8929-4d9277beb5dd",
   "metadata": {},
   "source": [
    "### tensor.norm的分析使用\n",
    "这里主要介绍一些对tensor.norm()的分析和使用。官方文档中给出的使用格式如下:\n",
    "```\n",
    "torch.norm(input, p='fro', dim=None, keepdim=False, out=None, dtype=None)\n",
    "```\n",
    "先介绍一些函数的主要功能：tensor.norm()主要用于计算向量范数或者矩阵范数的，其中p给定的就是计算几范数。  \n",
    "p可以给的数值：int, float, inf, -inf, 'fro', 'nuc', optional  \n",
    "下面给出一个计算向量二范数的例子："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "afc401df-0b29-402e-b0cf-d2af453039cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(5.)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([3,4],dtype=torch.float)\n",
    "x.requires_grad = True\n",
    "x = x.data.norm(p=2) # 等价于 torch.norm(x,p=2)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "868d0f54-6f13-4b46-ba82-daef37020877",
   "metadata": {},
   "source": [
    "上述代码进行了向量的二范数计算，其中p设置为2。需要注意的一点是:**只有float类型的tensor才可以进行requires_grad设置进而来计算梯度**，对int类型是不支持的.  \n",
    "从上述代码可以看出x.data.norm(p=2) 是等价于 torch.norm(x,p=2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
