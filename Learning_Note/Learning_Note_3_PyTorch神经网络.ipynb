{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7465b672-e117-40bf-a356-3e963158ddbe",
   "metadata": {},
   "source": [
    "# PyTorch神经网络\n",
    "在pytorch中，神经网络使用torch.nn来构建。神经网络是基于自动梯度来定义的一些模型，一个nn.Moudle包括层和一个方法forward()，它会返回输出。  \n",
    "\n",
    "比如：看下面数字图片识别网络:\n",
    "\n",
    "![figure.1](https://gitee.com/zyp521/upload_image/raw/master/oDyZLc.png)\n",
    "\n",
    "这是一个简单的前馈神经网络，它接收输入，让输出一个接着一个的通过一些层，最后给出输出。  \n",
    "\n",
    "一个经典的神经网络训练过程包含下列几点:\n",
    "- (1)定义一个包含可训练参数的神经网络\n",
    "- (2)迭代整个输入\n",
    "- (3)通过神经网络处理输入\n",
    "- (4)计算损失loss\n",
    "- (5)反向传播梯度到神经网络的参数\n",
    "- (6)更新网络的参数，比较经典的参数更新方法为:\n",
    "```\n",
    "weight = weight - learning_rate * gradient # learning_rate表示对梯度的接受程度\n",
    "```\n",
    "\n",
    "## 模型定义\n",
    "下面来定义一个神经网络:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "7c3aa666-0675-4cb0-818d-8062dc7736cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    # fixme: 网络搭建\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # build model\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # input channel为1 output channel 为 6, 5×5 kernal的 square convolution\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5) # I为6 O为16 kernal为5×5\n",
    "        # fc layer\n",
    "        self.fc1 = nn.Linear(16*5*5, 120) # Input为400 Ouput为120\n",
    "        self.fc2 = nn.Linear(120, 84) # Input为120 Ouput为84\n",
    "        self.fc3 = nn.Linear(84, 10) # Input为84 Output为10\n",
    "    \n",
    "    # fixme: 扁平化\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # batch数量\n",
    "        num_features = 1\n",
    "        # 按照batch来划分\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    # fixme: 损失函数\n",
    "    def loss(self, output, target):\n",
    "        criterion =  nn.MSELoss() # 调用MESLoss计算loss\n",
    "        return criterion(output, target)\n",
    "\n",
    "    # fixme: 前向传播\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        :param x: 输入图片\n",
    "        \"\"\"\n",
    "        # 卷积层1\n",
    "        x = self.conv1(x)\n",
    "        # activation function\n",
    "        x = F.relu(x)\n",
    "        # max pooling 2×2 window\n",
    "        x = F.max_pool2d(x,(2,2))\n",
    "        # 卷积层2\n",
    "        x = self.conv2(x)\n",
    "        # activation\n",
    "        x = F.relu(x)\n",
    "        # max pooling 2×2\n",
    "        x = F.max_pool2d(x,2) # 这里输入2，如果是2×2的window的话，则等价于(2,2)\n",
    "        # flatten\n",
    "        x = x.view(-1, self.num_flat_features(x)) # 将特征拉成一条\n",
    "        # Fc Layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "net = Net()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "535aa15a-8b41-4a19-8a51-7f6c2dfc28ee",
   "metadata": {},
   "source": [
    "这里定义了一个前馈函数，然后反向传播函数被自动通过autograd定义了。你可以使用任何张量操作在前馈函数上。  \n",
    "一个模型可训练的参数可以通过调用net.parameters()返回:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b58ded58-0969-445c-aa83-d012c82ee687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([6])\n",
      "torch.Size([16, 6, 5, 5])\n",
      "torch.Size([16])\n",
      "torch.Size([120, 400])\n",
      "torch.Size([120])\n",
      "torch.Size([84, 120])\n",
      "torch.Size([84])\n",
      "torch.Size([10, 84])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "#print(params)\n",
    "for i in range(len(params)):\n",
    "    print(params[i].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbb91bdb-5175-4bbb-9169-2ff44e854700",
   "metadata": {},
   "source": [
    "这里尝试随机生成32×32的输入，输入的维度是32×32. 为了能够让数据集在该网络上运行，需要将图片的维度修改为32×32.\n",
    "\n",
    "## 正向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "68c55531-4b1b-410b-84f9-157f8472fcea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0466,  0.1115, -0.0826,  0.1214,  0.0293, -0.0469, -0.1374, -0.2011,\n",
      "          0.0472,  0.0341],\n",
      "        [ 0.0588,  0.1029, -0.0577,  0.1265,  0.0065, -0.0465, -0.1586, -0.2081,\n",
      "          0.0639,  0.0531],\n",
      "        [ 0.0581,  0.1173, -0.0858,  0.1199,  0.0103, -0.0534, -0.1621, -0.2184,\n",
      "          0.0457,  0.0284],\n",
      "        [ 0.0475,  0.1148, -0.0773,  0.1166,  0.0198, -0.0303, -0.1578, -0.2353,\n",
      "          0.0562,  0.0482]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(4, 1, 32, 32) # 生成维度为1×32×32 batch中的元素个数为4\n",
    "input1 = torch.randn(1, 1, 32, 32) # 生成维度为1×32×32 batch中的元素个数为1\n",
    "# print(input)\n",
    "output = net(input)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26eee8d3-378a-4a57-a7f2-7297a3bdc221",
   "metadata": {},
   "source": [
    "针对上面网络中一些函数的分析:\n",
    "- (1) nn.Conv2d(x, y, z) 卷积层：构建一个接收通道数为x，输出通道数为y，kernel大小为 z×z\n",
    "- (2) nn.Linear(m, n) 线性变换：输入维度为m，输出的维度为n\n",
    "- (3) F.max_pool2d(x,m) max pooling：输入tensor为x，池化window的大小为m×m\n",
    "- (4) F.max_pool2d(x,(m,n)) 和上面同理，这里(m,n) 当m=n时，(m,m) 等价于 m\n",
    "- (5) F.relu(x) 激活函数\n",
    "- (6) tensor.view(m, n) 将tensor重新变形，但是m×n 要和tensor原来的维度相同\n",
    "- (7) params = list(net.parameters())得到每层对应的参数列表，params[0]对应一层，卷积层，激活函数都会被包含，但是max_pooling没有被包含\n",
    "\n",
    "对于输入\n",
    "```\n",
    "input1 = torch.randn(1, 1, 32, 32)\n",
    "```\n",
    "的分析，这里因为网络要求的输入channel为1，因此输入的维度为1×32×32，对于前面的1这里指的是一个batch的大小。  \n",
    "\n",
    "把所有的参数梯度缓存器置零，用随机的梯度来反向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8c15ef9a-3a26-4823-9754-f129c5879a74",
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad() # 网络所有参数梯度置零\n",
    "output.backward(torch.randn(4, 10)) # 4指的是batch的大小"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40443f5-d0c4-4d13-ba57-8a9fca111faa",
   "metadata": {},
   "source": [
    "上述工作做完之后，完成了下述任务：\n",
    "- (1)定义了一个神经网络\n",
    "- (2)正向传播以及调用反向传播\n",
    "\n",
    "还需要完成:\n",
    "- (1)计算loss\n",
    "- (2)更新网络参数权重\n",
    "\n",
    "## 反向传播\n",
    "### 计算loss\n",
    "计算loss代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "030cae2a-e213-4116-808f-b4b60bc4aa32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "tensor(0.9781, grad_fn=<MseLossBackward>)\n",
      "tensor(1.0270, grad_fn=<MseLossBackward>)\n",
      "tensor(1.2662, grad_fn=<MseLossBackward>)\n",
      "tensor(0.6900, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "output = net(input)\n",
    "print(output.size())\n",
    "target = torch.randn(4,10) # 生成4个batch对应的target\n",
    "for i in range(len(target)):\n",
    "    temp = target[i].view(-1,1) # 针对每个batch对应扁平化\n",
    "    res = net.loss(output[i], temp) # 计算loss\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa945c4-0170-4658-985c-686551aa1442",
   "metadata": {},
   "source": [
    "当我们调用loss.backward()，整个图都会微分，而且所有在图中的requires_grad=True的张量将会让它们的grad张量累计梯度。  \n",
    "\n",
    "反向传播例子代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "fa37f487-5500-4239-b76e-190bb8230764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 10])\n",
      "反向传播之前的梯度:\n",
      "None\n",
      "反向传播之后的梯度\n",
      "tensor([ 0.0003,  0.0011, -0.0007, -0.0008, -0.0002, -0.0008])\n"
     ]
    }
   ],
   "source": [
    "# 正向传播\n",
    "output = net(input1)\n",
    "print(output.size())\n",
    "target = torch.randn(1,10)\n",
    "temp = target.view(-1,1)\n",
    "loss = net.loss(output, temp) # 计算loss\n",
    "\n",
    "# 反向传播\n",
    "net.zero_grad() # 将所有张量的梯度换层置零\n",
    "print('反向传播之前的梯度:')\n",
    "print(net.conv1.bias.grad)\n",
    "\n",
    "loss.backward() # 反向传播\n",
    "\n",
    "print('反向传播之后的梯度')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe20918e-1017-4b49-a15d-519be2c4d0c1",
   "metadata": {},
   "source": [
    "### 更新网络参数\n",
    "这里使用一个比较简单的更新规则来进行参数的更新:\n",
    "```\n",
    "weight = weight - learning_rate * gradient\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d5886b-34a3-47a1-a083-30a1f4484cb6",
   "metadata": {},
   "source": [
    "使用torch.optim实现了所有的方法。这里直接调用包来进行优化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7d92e3d6-0f7a-4a69-99a1-243108564417",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# 创建优化器\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01) # 使用随机梯度下降\n",
    "optimizer.zero_grad() # 梯度置零\n",
    "# 前向传播\n",
    "input1 = torch.randn(1, 1, 32, 32)\n",
    "output = net(input1)\n",
    "target = torch.randn(10)\n",
    "# 计算loss\n",
    "loss = net.loss(output,target)\n",
    "# 反向传播\n",
    "loss.backward()\n",
    "optimizer.step() # 参数梯度更新"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
