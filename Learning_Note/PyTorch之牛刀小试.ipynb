{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f45b993a-1ed4-487d-9fd6-05c1d0123e31",
   "metadata": {},
   "source": [
    "# PyTorch之牛刀小试\n",
    "- (1)一个n张量，类似于Numpy，但可以在GPU上运行\n",
    "- (2)搭建和训练神经网络时的自动微分/求导机制\n",
    "\n",
    "## 张量\n",
    "Numpy是一个很棒的框架，但它不能利用GPU来加速其数值计算。 对于现代深度神经网络，GPU通常提供50倍或更高的加速，所以，numpy不能满足当代深度学习的需求。\n",
    "\n",
    "在这里先介绍一下PyTorch的概念:  \n",
    "\n",
    "张量(Tensor)：PyTorch的tensor在概念上与numpy的array相同，tensor是一个n维数组，PyTorch提供了许多函数用于操作这些张量。任何希望使用Numpy执行的计算也可以使用PyTorch的tensor来完成，可以认为它们是科学计算的通用工具。  \n",
    "\n",
    "但是与Numpy不同的是，PyTorch可以利用GPU加速其数值计算，要在GPU上运行Tensor，在构造张量使用device参数把tensor建立在GPU上。在这里，使用tensor将随机数据上训练一个两层的网络，这里先手动实现前向传播和反向传播:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8ff4ebb-3617-4dce-b20f-ce22288ea04f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前代数:0，当前loss为:31054268.0\n",
      "当前代数:1，当前loss为:32792080.0\n",
      "当前代数:2，当前loss为:40690232.0\n",
      "当前代数:3，当前loss为:47072136.0\n",
      "当前代数:4，当前loss为:43159876.0\n",
      "当前代数:5，当前loss为:27847366.0\n",
      "当前代数:6，当前loss为:12906761.0\n",
      "当前代数:7，当前loss为:5127627.0\n",
      "当前代数:8，当前loss为:2330071.5\n",
      "当前代数:9，当前loss为:1383562.0\n",
      "当前代数:10，当前loss为:1009468.875\n",
      "当前代数:11，当前loss为:813189.875\n",
      "当前代数:12，当前loss为:682103.75\n",
      "当前代数:13，当前loss为:582411.75\n",
      "当前代数:14，当前loss为:502093.40625\n",
      "当前代数:15，当前loss为:435708.875\n",
      "当前代数:16，当前loss为:380028.4375\n",
      "当前代数:17，当前loss为:332927.09375\n",
      "当前代数:18，当前loss为:292841.8125\n",
      "当前代数:19，当前loss为:258505.203125\n",
      "当前代数:20，当前loss为:228938.359375\n",
      "当前代数:21，当前loss为:203353.5\n",
      "当前代数:22，当前loss为:181149.84375\n",
      "当前代数:23，当前loss为:161809.625\n",
      "当前代数:24，当前loss为:144882.734375\n",
      "当前代数:25，当前loss为:130003.984375\n",
      "当前代数:26，当前loss为:116890.46875\n",
      "当前代数:27，当前loss为:105295.953125\n",
      "当前代数:28，当前loss为:95030.5625\n",
      "当前代数:29，当前loss为:85919.65625\n",
      "当前代数:30，当前loss为:77802.578125\n",
      "当前代数:31，当前loss为:70554.015625\n",
      "当前代数:32，当前loss为:64070.0390625\n",
      "当前代数:33，当前loss为:58257.4375\n",
      "当前代数:34，当前loss为:53037.4765625\n",
      "当前代数:35，当前loss为:48341.96875\n",
      "当前代数:36，当前loss为:44109.859375\n",
      "当前代数:37，当前loss为:40289.40625\n",
      "当前代数:38，当前loss为:36837.1953125\n",
      "当前代数:39，当前loss为:33713.8828125\n",
      "当前代数:40，当前loss为:30887.244140625\n",
      "当前代数:41，当前loss为:28321.28125\n",
      "当前代数:42，当前loss为:25990.703125\n",
      "当前代数:43，当前loss为:23873.1328125\n",
      "当前代数:44，当前loss为:21947.1640625\n",
      "当前代数:45，当前loss为:20193.380859375\n",
      "当前代数:46，当前loss为:18593.29296875\n",
      "当前代数:47，当前loss为:17131.26171875\n",
      "当前代数:48，当前loss为:15794.61328125\n",
      "当前代数:49，当前loss为:14571.60546875\n",
      "当前代数:50，当前loss为:13452.8427734375\n",
      "当前代数:51，当前loss为:12427.859375\n",
      "当前代数:52，当前loss为:11490.49609375\n",
      "当前代数:53，当前loss为:10629.93359375\n",
      "当前代数:54，当前loss为:9839.3974609375\n",
      "当前代数:55，当前loss为:9115.361328125\n",
      "当前代数:56，当前loss为:8449.47265625\n",
      "当前代数:57，当前loss为:7836.5732421875\n",
      "当前代数:58，当前loss为:7271.9189453125\n",
      "当前代数:59，当前loss为:6753.701171875\n",
      "当前代数:60，当前loss为:6276.8466796875\n",
      "当前代数:61，当前loss为:5836.46484375\n",
      "当前代数:62，当前loss为:5429.6513671875\n",
      "当前代数:63，当前loss为:5053.68408203125\n",
      "当前代数:64，当前loss为:4705.91259765625\n",
      "当前代数:65，当前loss为:4384.21875\n",
      "当前代数:66，当前loss为:4086.33984375\n",
      "当前代数:67，当前loss为:3810.31396484375\n",
      "当前代数:68，当前loss为:3554.549072265625\n",
      "当前代数:69，当前loss为:3317.328125\n",
      "当前代数:70，当前loss为:3097.296875\n",
      "当前代数:71，当前loss为:2893.00390625\n",
      "当前代数:72，当前loss为:2703.26513671875\n",
      "当前代数:73，当前loss为:2526.984375\n",
      "当前代数:74，当前loss为:2363.15625\n",
      "当前代数:75，当前loss为:2210.6796875\n",
      "当前代数:76，当前loss为:2068.904541015625\n",
      "当前代数:77，当前loss为:1936.98974609375\n",
      "当前代数:78，当前loss为:1814.16650390625\n",
      "当前代数:79，当前loss为:1699.7532958984375\n",
      "当前代数:80，当前loss为:1593.081298828125\n",
      "当前代数:81，当前loss为:1493.680908203125\n",
      "当前代数:82，当前loss为:1400.91455078125\n",
      "当前代数:83，当前loss为:1314.3193359375\n",
      "当前代数:84，当前loss为:1233.517578125\n",
      "当前代数:85，当前loss为:1158.0687255859375\n",
      "当前代数:86，当前loss为:1087.505126953125\n",
      "当前代数:87，当前loss为:1021.5667724609375\n",
      "当前代数:88，当前loss为:959.9125366210938\n",
      "当前代数:89，当前loss为:902.2371215820312\n",
      "当前代数:90，当前loss为:848.255615234375\n",
      "当前代数:91，当前loss为:797.7470092773438\n",
      "当前代数:92，当前loss为:750.476806640625\n",
      "当前代数:93，当前loss为:706.16162109375\n",
      "当前代数:94，当前loss为:664.6378173828125\n",
      "当前代数:95，当前loss为:625.716552734375\n",
      "当前代数:96，当前loss为:589.2378540039062\n",
      "当前代数:97，当前loss为:555.0079956054688\n",
      "当前代数:98，当前loss为:522.9053955078125\n",
      "当前代数:99，当前loss为:492.782958984375\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# fixme: 初始配置\n",
    "dtype = torch.float\n",
    "# device = torch.device(\"cpu\")＃取消注释以在CPU上运\n",
    "device = torch.device('cuda:0')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N为批量大小，D_in是输入维度，H是隐藏层维度，D_out是输出层维度 (这里构建的两层网络)\n",
    "learning_rate = 1e-6 # 学习速率\n",
    "epochs = 100 # 代数\n",
    "\n",
    "# fixme: 创建随机输入和输出\n",
    "input = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "label = torch.randn(N, D_out, device=device,dtype=dtype)\n",
    "\n",
    "# fixme: 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device,dtype=dtype)\n",
    "\n",
    "# fixme: 训练\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播部分\n",
    "    h = input.mm(w1) # 输入层到隐藏层\n",
    "    h_relu = h.clamp(min=0) # 类似relu函数\n",
    "    pred = h_relu.mm(w2) # 隐藏层到输出层\n",
    "    \n",
    "    # 计算损失并且打印损失\n",
    "    loss = (pred - label).pow(2).sum().item() # 差的平方求和\n",
    "    # print('当前代数:',epoch,'，当前loss为:',loss)\n",
    "    print('当前代数:{}，当前loss为:{}'.format(epoch,loss))\n",
    "    \n",
    "    # 反向传播，计算梯度(推导公式模式识别里面讲过)\n",
    "    grad_pre = 2.0 * (pred - label)\n",
    "    grad_w2 = h_relu.t().mm(grad_pre)\n",
    "    \n",
    "    grad_h_relu = grad_pre.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    \n",
    "    grad_w1 = input.t().mm(grad_h)\n",
    "    \n",
    "    # 使用学习速率更新权重\n",
    "    w1 -= learning_rate * grad_w1\n",
    "    w2 -= learning_rate * grad_w2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc64850-9e95-4b09-a800-fea7c6488ddc",
   "metadata": {},
   "source": [
    "## 自动求导\n",
    "### 张量和自动求导\n",
    "上面的例子中，我们手动实现了前向传播和后向传播，手动实现对于小型的网络来说还是可行的，但是如果对于实现大型复杂的网络来说那将变得十分复杂，但是可以使用自动微分来自动计算神经网络中的后向传播。  \n",
    "PyTorch中的autograd包提供了这个功能，**当使用autograd时，网络前向传播将定义一个计算图，图中的节点是tensor，边是函数，这些函数是输出tensor到输入tensor的映射**。这张计算图使得在网络中反向传播时梯度的计算十分简单。  \n",
    "\n",
    "上面的描述可能听起来十分复杂，但是在实践中非常简单，我们如果想要计算某些tensor的梯度，我们只需要在建立这个tensor时加入一句： **requires_grad=True**，这个tensor上的任何PyTorch的操作都将构建一个计算图，从而允许我们稍后在图中执行反向传播。如果这个tensor x的requires_grad=True，那么反向传播之后，x.grad将会是另一个张量，其为x关于某个标量值的梯度。  \n",
    "有时可能希望防止PyTorch在requires_grad=True的张量执行某些操作时，构建计算图；例如，在训练神经网络时，我们通常不希望通过权重更新步骤进行反向传播，这种情况下，我们可以使用torch.no_grad()上下文管理器来防止构建计算图。\n",
    "\n",
    "**默认情况下,所有的tensor的requires_grad均为True**\n",
    "\n",
    "下面是一个例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a22d9309-1aaa-4c9a-b0a5-2e9a0ba40630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前代数:0，当前loss为:27810032.0\n",
      "当前代数:1，当前loss为:24036258.0\n",
      "当前代数:2，当前loss为:22474504.0\n",
      "当前代数:3，当前loss为:20527414.0\n",
      "当前代数:4，当前loss为:17089804.0\n",
      "当前代数:5，当前loss为:12828048.0\n",
      "当前代数:6，当前loss为:8727194.0\n",
      "当前代数:7，当前loss为:5624930.0\n",
      "当前代数:8，当前loss为:3571583.75\n",
      "当前代数:9，当前loss为:2328650.5\n",
      "当前代数:10，当前loss为:1592575.375\n",
      "当前代数:11，当前loss为:1153773.75\n",
      "当前代数:12，当前loss为:881616.625\n",
      "当前代数:13，当前loss为:703650.875\n",
      "当前代数:14，当前loss为:580100.5625\n",
      "当前代数:15，当前loss为:489431.375\n",
      "当前代数:16，当前loss为:419695.375\n",
      "当前代数:17，当前loss为:364094.375\n",
      "当前代数:18，当前loss为:318578.28125\n",
      "当前代数:19，当前loss为:280553.78125\n",
      "当前代数:20，当前loss为:248364.734375\n",
      "当前代数:21，当前loss为:220813.15625\n",
      "当前代数:22，当前loss为:197072.859375\n",
      "当前代数:23，当前loss为:176429.609375\n",
      "当前代数:24，当前loss为:158397.40625\n",
      "当前代数:25，当前loss为:142566.15625\n",
      "当前代数:26，当前loss为:128629.28125\n",
      "当前代数:27，当前loss为:116309.9765625\n",
      "当前代数:28，当前loss为:105392.078125\n",
      "当前代数:29，当前loss为:95699.6875\n",
      "当前代数:30，当前loss为:87067.65625\n",
      "当前代数:31，当前loss为:79355.140625\n",
      "当前代数:32，当前loss为:72449.046875\n",
      "当前代数:33，当前loss为:66244.546875\n",
      "当前代数:34，当前loss为:60659.79296875\n",
      "当前代数:35，当前loss为:55621.03515625\n",
      "当前代数:36，当前loss为:51073.1875\n",
      "当前代数:37，当前loss为:46961.421875\n",
      "当前代数:38，当前loss为:43233.93359375\n",
      "当前代数:39，当前loss为:39850.5859375\n",
      "当前代数:40，当前loss为:36773.32421875\n",
      "当前代数:41，当前loss为:33976.8671875\n",
      "当前代数:42，当前loss为:31429.7109375\n",
      "当前代数:43，当前loss为:29104.251953125\n",
      "当前代数:44，当前loss为:26976.142578125\n",
      "当前代数:45，当前loss为:25027.2890625\n",
      "当前代数:46，当前loss为:23240.3515625\n",
      "当前代数:47，当前loss为:21601.375\n",
      "当前代数:48，当前loss为:20093.41015625\n",
      "当前代数:49，当前loss为:18705.25\n",
      "当前代数:50，当前loss为:17425.64453125\n",
      "当前代数:51，当前loss为:16245.494140625\n",
      "当前代数:52，当前loss为:15155.6767578125\n",
      "当前代数:53，当前loss为:14147.82421875\n",
      "当前代数:54，当前loss为:13215.876953125\n",
      "当前代数:55，当前loss为:12352.66796875\n",
      "当前代数:56，当前loss为:11552.3515625\n",
      "当前代数:57，当前loss为:10810.265625\n",
      "当前代数:58，当前loss为:10121.189453125\n",
      "当前代数:59，当前loss为:9481.2197265625\n",
      "当前代数:60，当前loss为:8886.361328125\n",
      "当前代数:61，当前loss为:8333.0244140625\n",
      "当前代数:62，当前loss为:7817.830078125\n",
      "当前代数:63，当前loss为:7337.8115234375\n",
      "当前代数:64，当前loss为:6890.5537109375\n",
      "当前代数:65，当前loss为:6473.25341796875\n",
      "当前代数:66，当前loss为:6083.50732421875\n",
      "当前代数:67，当前loss为:5719.70947265625\n",
      "当前代数:68，当前loss为:5379.83203125\n",
      "当前代数:69，当前loss为:5062.0498046875\n",
      "当前代数:70，当前loss为:4764.7119140625\n",
      "当前代数:71，当前loss为:4486.591796875\n",
      "当前代数:72，当前loss为:4226.27197265625\n",
      "当前代数:73，当前loss为:3982.27294921875\n",
      "当前代数:74，当前loss为:3753.611083984375\n",
      "当前代数:75，当前loss为:3539.224365234375\n",
      "当前代数:76，当前loss为:3338.146728515625\n",
      "当前代数:77，当前loss为:3149.380126953125\n",
      "当前代数:78，当前loss为:2972.0908203125\n",
      "当前代数:79，当前loss为:2805.58544921875\n",
      "当前代数:80，当前loss为:2649.118896484375\n",
      "当前代数:81，当前loss为:2502.1630859375\n",
      "当前代数:82，当前loss为:2363.88671875\n",
      "当前代数:83，当前loss为:2233.85888671875\n",
      "当前代数:84，当前loss为:2111.525390625\n",
      "当前代数:85，当前loss为:1996.379638671875\n",
      "当前代数:86，当前loss为:1887.927001953125\n",
      "当前代数:87，当前loss为:1785.772705078125\n",
      "当前代数:88，当前loss为:1689.510009765625\n",
      "当前代数:89，当前loss为:1598.76806640625\n",
      "当前代数:90，当前loss为:1513.199462890625\n",
      "当前代数:91，当前loss为:1432.513671875\n",
      "当前代数:92，当前loss为:1356.64013671875\n",
      "当前代数:93，当前loss为:1284.995849609375\n",
      "当前代数:94，当前loss为:1217.4063720703125\n",
      "当前代数:95，当前loss为:1153.574951171875\n",
      "当前代数:96，当前loss为:1093.319580078125\n",
      "当前代数:97，当前loss为:1036.3707275390625\n",
      "当前代数:98，当前loss为:982.5541381835938\n",
      "当前代数:99，当前loss为:931.6908569335938\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# fixme: 配置\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N为批量大小，D_in是输入维度，H是隐藏层维度，D_out是输出层维度 (这里构建的两层网络)\n",
    "learning_rate = 1e-6 # 学习速率\n",
    "epochs = 100 # 代数\n",
    "# fixme: 创建随机输入和输出\n",
    "input = torch.randn(N, D_in, device=device, dtype=dtype, requires_grad=True)\n",
    "label = torch.randn(N, D_out, device=device,dtype=dtype, requires_grad=True)\n",
    "\n",
    "# fixme: 随机初始化权重\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device,dtype=dtype, requires_grad=True)\n",
    "\n",
    "# fixme: 训练\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播部分\n",
    "    h = input.mm(w1) # 输入层到隐藏层\n",
    "    h_relu = h.clamp(min=0) # 类似relu函数\n",
    "    pred = h_relu.mm(w2) # 隐藏层到输出层\n",
    "    \n",
    "    # 计算损失并且打印损失\n",
    "    loss = (pred - label).pow(2).sum() # 差的平方求和\n",
    "    # print('当前代数:',epoch,'，当前loss为:',loss)\n",
    "    print('当前代数:{}，当前loss为:{}'.format(epoch,loss.item()))\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新梯度\n",
    "    ## 不更新计算图\n",
    "    with torch.no_grad():\n",
    "        w1 -= learning_rate * w1.grad\n",
    "        w2 -= learning_rate * w2.grad\n",
    "        \n",
    "        # 手动将梯度设置为0\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "195a32b9-afec-4506-b849-1517045e9b3c",
   "metadata": {},
   "source": [
    "### 定义新的自动求导函数\n",
    "在底层，每一个原始的自动求导运算实际上是两个在Tensor上运行的函数。其中， forward 函数计算从输入Tensors获得的输出Tensors。而 backward 函数接收输出Tensors对于某个标量值的梯度，并且计算输入Tensors相对于该相同标量值的梯度。  \n",
    "\n",
    "在PyTorch中，我们可以很容易地通过定义 torch.autograd.Function 的子类并实现 forward 和backward函数，来定义自己的自动求导运算。之后我们就可以使用这个新的自动梯度运算符了。然后，我们可以通过构造一个实例并像调用函数一样，传入包含输入数据的tensor调用它，这样来使用新的自动求导运算。  \n",
    "\n",
    "下面是一个例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "167822b5-3fc5-487d-a455-295d6bfb1bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "        通过建立torch.autograd的子类来实现我们自定义的autograd函数，并且完成张量的正向和反向传播。\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def forward(ctx, x):\n",
    "        \"\"\"\n",
    "        在正向传播中，我们接收到一个上下文对象和一个包含输入的张量；\n",
    "        我们必须返回一个包含输出的张量，并且我们可以使用上下文对象来缓存对象，\n",
    "        以便在反向传播中使用。\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(x)\n",
    "        return x.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        在反向传播中，我们接收到上下文对象和一个张量，\n",
    "        其包含了相对于正向传播过程中产生的输出的损失的梯度。\n",
    "        我们可以从上下文对象中检索缓存的数据，\n",
    "        并且必须计算并返回与正向传播的输入相关的损失的梯度。\n",
    "        \"\"\"\n",
    "        x, = ctx.saved_tensors\n",
    "        grad_x = grad_output.clone()\n",
    "        grad_x[x < 0] = 0\n",
    "        return grad_x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d76860-5146-48a0-bec4-e18d0ed9159c",
   "metadata": {},
   "source": [
    "### TensorFlow：静态图\n",
    "PyTorch自动求导看起来非常像TensorFlow，这两个框架中，我们都定义了计算图，使用自动微分来计算梯度，但是两者之间最大的不同是TensorFlow的计算图是静态的，而PyTorch使用的是动态的计算图。    \n",
    "**在TensorFlow中，我们定义计算图一次，然后后续就会重复执行这个相同的图，后面的话可能只是会提供不同的输入数据，而在PyTorch中，每一个前向通道(forward)定义一个新的计算图。**  \n",
    "\n",
    "静态图的好处在于你可以预先对图进行优化。例如：一个框架可能要融合一些图的运算来提升效率，或者产生一个策略来将图分布到多个GPU或者机器上，如果重复使用相同的图，那么再重复运行一个图时，前期潜在的代价高昂的预先优化的消耗就会被分摊开。  \n",
    "\n",
    "静态图和动态图的一个区别是**控制流**。对于一些模型，我们希望对每个数据点执行不同的计算。例如：一个递归神经网络可能对每个数据点执行不同的时间步数，这个展开（unrolling)可以作为一个循环来实现。  \n",
    "对于一个静态图，循环结构要作为图的一部分，因此TensorFlow提供了运算符来把循环嵌入到图当中。对于动态图来说，情况更加简单，既然我们为每个例子即时创建计算图，我们可以使用普通的命令式控制流来为每个输入执行不同的计算。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeb18676-97ba-4fe7-9bfa-0bf524415e23",
   "metadata": {},
   "source": [
    "## nn模块\n",
    "### torch.nn\n",
    "计算图和autograd是十分强大的工具，可以定义复杂的操作并自动求导，然后对于大规模的网络来说，autograd又太过于底层了，在构建神经网络时，我们经常考虑将计算安排成层，其中一些具有可学习性的参数，让它们在学习过程中进行优化。  \n",
    "\n",
    "在TensorFlow中，有类似Keras，TensorFlow-Slim和TFLearn这种封装了底层计算图的高度抽象的接口，这使得构建网络十分方便。  \n",
    "\n",
    "在PyTorch中，包nn完成了和TFLearn等类似的功能，nn包中定义一组大致等价于层的块。一个模块接受输入的tensor，计算输出的tensor，而且还保存了一些内部状态，比如需要学习的tensor的参数等。nn包中也定义了一组损失函数，用来训练神经网络。\n",
    "\n",
    "下面给出一个用nn包实现两层网络的例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b613999-9270-44d2-a005-4f0931a9aac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前代数:0，当前loss为:694.401123046875\n",
      "当前代数:1，当前loss为:640.5441284179688\n",
      "当前代数:2，当前loss为:594.4757690429688\n",
      "当前代数:3，当前loss为:554.1473388671875\n",
      "当前代数:4，当前loss为:518.3447875976562\n",
      "当前代数:5，当前loss为:486.1473388671875\n",
      "当前代数:6，当前loss为:457.0733337402344\n",
      "当前代数:7，当前loss为:430.4808654785156\n",
      "当前代数:8，当前loss为:406.1338195800781\n",
      "当前代数:9，当前loss为:383.46478271484375\n",
      "当前代数:10，当前loss为:362.3720703125\n",
      "当前代数:11，当前loss为:342.73736572265625\n",
      "当前代数:12，当前loss为:324.1927185058594\n",
      "当前代数:13，当前loss为:306.5917053222656\n",
      "当前代数:14，当前loss为:289.88323974609375\n",
      "当前代数:15，当前loss为:274.0377197265625\n",
      "当前代数:16，当前loss为:259.0152587890625\n",
      "当前代数:17，当前loss为:244.71998596191406\n",
      "当前代数:18，当前loss为:231.1725311279297\n",
      "当前代数:19，当前loss为:218.3126983642578\n",
      "当前代数:20，当前loss为:206.06178283691406\n",
      "当前代数:21，当前loss为:194.44424438476562\n",
      "当前代数:22，当前loss为:183.42593383789062\n",
      "当前代数:23，当前loss为:172.9327392578125\n",
      "当前代数:24，当前loss为:162.96517944335938\n",
      "当前代数:25，当前loss为:153.50523376464844\n",
      "当前代数:26，当前loss为:144.53953552246094\n",
      "当前代数:27，当前loss为:136.04568481445312\n",
      "当前代数:28，当前loss为:128.0102996826172\n",
      "当前代数:29，当前loss为:120.40977478027344\n",
      "当前代数:30，当前loss为:113.23602294921875\n",
      "当前代数:31，当前loss为:106.4620361328125\n",
      "当前代数:32，当前loss为:100.07986450195312\n",
      "当前代数:33，当前loss为:94.066650390625\n",
      "当前代数:34，当前loss为:88.40013885498047\n",
      "当前代数:35，当前loss为:83.05750274658203\n",
      "当前代数:36，当前loss为:78.02630615234375\n",
      "当前代数:37，当前loss为:73.3027114868164\n",
      "当前代数:38，当前loss为:68.87252044677734\n",
      "当前代数:39，当前loss为:64.70759582519531\n",
      "当前代数:40，当前loss为:60.798988342285156\n",
      "当前代数:41，当前loss为:57.132286071777344\n",
      "当前代数:42，当前loss为:53.69049072265625\n",
      "当前代数:43，当前loss为:50.463531494140625\n",
      "当前代数:44，当前loss为:47.43849182128906\n",
      "当前代数:45，当前loss为:44.604164123535156\n",
      "当前代数:46，当前loss为:41.94725799560547\n",
      "当前代数:47，当前loss为:39.454227447509766\n",
      "当前代数:48，当前loss为:37.12156677246094\n",
      "当前代数:49，当前loss为:34.93732833862305\n",
      "当前代数:50，当前loss为:32.89033508300781\n",
      "当前代数:51，当前loss为:30.974105834960938\n",
      "当前代数:52，当前loss为:29.17722511291504\n",
      "当前代数:53，当前loss为:27.496543884277344\n",
      "当前代数:54，当前loss为:25.922353744506836\n",
      "当前代数:55，当前loss为:24.445026397705078\n",
      "当前代数:56，当前loss为:23.061433792114258\n",
      "当前代数:57，当前loss为:21.764814376831055\n",
      "当前代数:58，当前loss为:20.55123519897461\n",
      "当前代数:59，当前loss为:19.41389274597168\n",
      "当前代数:60，当前loss为:18.348012924194336\n",
      "当前代数:61，当前loss为:17.346460342407227\n",
      "当前代数:62，当前loss为:16.40635108947754\n",
      "当前代数:63，当前loss为:15.523895263671875\n",
      "当前代数:64，当前loss为:14.695244789123535\n",
      "当前代数:65，当前loss为:13.916027069091797\n",
      "当前代数:66，当前loss为:13.185911178588867\n",
      "当前代数:67，当前loss为:12.498588562011719\n",
      "当前代数:68，当前loss为:11.853001594543457\n",
      "当前代数:69，当前loss为:11.24429702758789\n",
      "当前代数:70，当前loss为:10.671463012695312\n",
      "当前代数:71，当前loss为:10.13284683227539\n",
      "当前代数:72，当前loss为:9.624886512756348\n",
      "当前代数:73，当前loss为:9.146224975585938\n",
      "当前代数:74，当前loss为:8.695545196533203\n",
      "当前代数:75，当前loss为:8.270209312438965\n",
      "当前代数:76，当前loss为:7.868492126464844\n",
      "当前代数:77，当前loss为:7.489187717437744\n",
      "当前代数:78，当前loss为:7.130546569824219\n",
      "当前代数:79，当前loss为:6.791640281677246\n",
      "当前代数:80，当前loss为:6.471275806427002\n",
      "当前代数:81，当前loss为:6.168827533721924\n",
      "当前代数:82，当前loss为:5.882907867431641\n",
      "当前代数:83，当前loss为:5.611786842346191\n",
      "当前代数:84，当前loss为:5.3550896644592285\n",
      "当前代数:85，当前loss为:5.111222267150879\n",
      "当前代数:86，当前loss为:4.87969970703125\n",
      "当前代数:87，当前loss为:4.659515857696533\n",
      "当前代数:88，当前loss为:4.451183319091797\n",
      "当前代数:89，当前loss为:4.253047466278076\n",
      "当前代数:90，当前loss为:4.064755439758301\n",
      "当前代数:91，当前loss为:3.885852336883545\n",
      "当前代数:92，当前loss为:3.7156379222869873\n",
      "当前代数:93，当前loss为:3.5539867877960205\n",
      "当前代数:94，当前loss为:3.400294780731201\n",
      "当前代数:95，当前loss为:3.253854513168335\n",
      "当前代数:96，当前loss为:3.114656448364258\n",
      "当前代数:97，当前loss为:2.9821271896362305\n",
      "当前代数:98，当前loss为:2.8556747436523438\n",
      "当前代数:99，当前loss为:2.735225200653076\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# fixme: 参数配置\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N为批量大小，D_in是输入维度，H是隐藏层维度，D_out是输出层维度\n",
    "learning_rate = 1e-4\n",
    "epoches = 100\n",
    "\n",
    "# fixeme: 创建输入和输出随机张量\n",
    "input = torch.randn(N, D_in)\n",
    "label = torch.randn(N, D_out)\n",
    "\n",
    "# fixme: 使用nn包将我们的模型定义为一系列的层\n",
    "## nn.Sequential是包含其他模块的模块，并按顺序应用这些模块来产生其输出。\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# nn包还包含常用的损失函数的定义\n",
    "# 在这种情况下，我们将使用平均平方误差(MSE)作为我们的损失函数\n",
    "# 设置reduction='sum'，表示我们计算的是平方误差的“和”，而不是平均值;\n",
    "# 但是在实践中，通过设置reduction='elementwise_mean'来使用均方误差作为损失更为常见。\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "for epoch in range(epoches):\n",
    "    # 前向传播\n",
    "    pred = model(input)\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = loss_fn(pred, label)\n",
    "    print('当前代数:{}，当前loss为:{}'.format(epoch,loss.item()))\n",
    "    \n",
    "    # 反向传播梯度清零\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    \n",
    "    # 更新梯度\n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= learning_rate * param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1ca5ba-86a9-4bdc-90f0-adf581e35734",
   "metadata": {},
   "source": [
    "### torch.optim\n",
    "到目前为止，我们已经通过手动改变包含可学习参数的张量来更新模型的权重。对于随机梯度下降(SGD/stochastic gradient descent)等简单的优化算法来说，这不是一个很大的负担，但在实践中，我们经常使用AdaGrad、RMSProp、Adam等更复杂的优化器来训练神经网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f86628a-061d-41c7-a54a-018038ca4bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前代数:0，当前loss为:671.5050048828125\n",
      "当前代数:1，当前loss为:654.7916870117188\n",
      "当前代数:2，当前loss为:638.5562133789062\n",
      "当前代数:3，当前loss为:622.7379760742188\n",
      "当前代数:4，当前loss为:607.2938842773438\n",
      "当前代数:5，当前loss为:592.3021850585938\n",
      "当前代数:6，当前loss为:577.6829223632812\n",
      "当前代数:7，当前loss为:563.436279296875\n",
      "当前代数:8，当前loss为:549.675048828125\n",
      "当前代数:9，当前loss为:536.265869140625\n",
      "当前代数:10，当前loss为:523.209228515625\n",
      "当前代数:11，当前loss为:510.4918212890625\n",
      "当前代数:12，当前loss为:498.1060791015625\n",
      "当前代数:13，当前loss为:486.1214599609375\n",
      "当前代数:14，当前loss为:474.44940185546875\n",
      "当前代数:15，当前loss为:463.1405029296875\n",
      "当前代数:16，当前loss为:452.1324157714844\n",
      "当前代数:17，当前loss为:441.3900146484375\n",
      "当前代数:18，当前loss为:431.0217590332031\n",
      "当前代数:19，当前loss为:420.8746337890625\n",
      "当前代数:20，当前loss为:410.96966552734375\n",
      "当前代数:21，当前loss为:401.34454345703125\n",
      "当前代数:22，当前loss为:391.9026794433594\n",
      "当前代数:23，当前loss为:382.7084655761719\n",
      "当前代数:24，当前loss为:373.7532043457031\n",
      "当前代数:25，当前loss为:364.9910888671875\n",
      "当前代数:26，当前loss为:356.41558837890625\n",
      "当前代数:27，当前loss为:348.0168762207031\n",
      "当前代数:28，当前loss为:339.8182067871094\n",
      "当前代数:29，当前loss为:331.8048095703125\n",
      "当前代数:30，当前loss为:323.9619445800781\n",
      "当前代数:31，当前loss为:316.2928466796875\n",
      "当前代数:32，当前loss为:308.77508544921875\n",
      "当前代数:33，当前loss为:301.38861083984375\n",
      "当前代数:34，当前loss为:294.1795654296875\n",
      "当前代数:35，当前loss为:287.1237487792969\n",
      "当前代数:36，当前loss为:280.240966796875\n",
      "当前代数:37，当前loss为:273.47882080078125\n",
      "当前代数:38，当前loss为:266.8583679199219\n",
      "当前代数:39，当前loss为:260.3664855957031\n",
      "当前代数:40，当前loss为:254.00315856933594\n",
      "当前代数:41，当前loss为:247.7609405517578\n",
      "当前代数:42，当前loss为:241.64744567871094\n",
      "当前代数:43，当前loss为:235.6461181640625\n",
      "当前代数:44，当前loss为:229.7653045654297\n",
      "当前代数:45，当前loss为:224.0077362060547\n",
      "当前代数:46，当前loss为:218.3994598388672\n",
      "当前代数:47，当前loss为:212.91940307617188\n",
      "当前代数:48，当前loss为:207.53265380859375\n",
      "当前代数:49，当前loss为:202.24990844726562\n",
      "当前代数:50，当前loss为:197.06724548339844\n",
      "当前代数:51，当前loss为:191.98426818847656\n",
      "当前代数:52，当前loss为:186.99029541015625\n",
      "当前代数:53，当前loss为:182.10096740722656\n",
      "当前代数:54，当前loss为:177.32080078125\n",
      "当前代数:55，当前loss为:172.63706970214844\n",
      "当前代数:56，当前loss为:168.04400634765625\n",
      "当前代数:57，当前loss为:163.5508575439453\n",
      "当前代数:58，当前loss为:159.14199829101562\n",
      "当前代数:59，当前loss为:154.81480407714844\n",
      "当前代数:60，当前loss为:150.5882568359375\n",
      "当前代数:61，当前loss为:146.44244384765625\n",
      "当前代数:62，当前loss为:142.37423706054688\n",
      "当前代数:63，当前loss为:138.40367126464844\n",
      "当前代数:64，当前loss为:134.52549743652344\n",
      "当前代数:65，当前loss为:130.73109436035156\n",
      "当前代数:66，当前loss为:127.02294921875\n",
      "当前代数:67，当前loss为:123.40174102783203\n",
      "当前代数:68，当前loss为:119.85633850097656\n",
      "当前代数:69，当前loss为:116.3910903930664\n",
      "当前代数:70，当前loss为:112.99314880371094\n",
      "当前代数:71，当前loss为:109.68431091308594\n",
      "当前代数:72，当前loss为:106.45740509033203\n",
      "当前代数:73，当前loss为:103.3082046508789\n",
      "当前代数:74，当前loss为:100.23359680175781\n",
      "当前代数:75，当前loss为:97.23468780517578\n",
      "当前代数:76，当前loss为:94.31403350830078\n",
      "当前代数:77，当前loss为:91.46961212158203\n",
      "当前代数:78，当前loss为:88.6979751586914\n",
      "当前代数:79，当前loss为:85.99103546142578\n",
      "当前代数:80，当前loss为:83.34943389892578\n",
      "当前代数:81，当前loss为:80.77326202392578\n",
      "当前代数:82，当前loss为:78.2638931274414\n",
      "当前代数:83，当前loss为:75.81172180175781\n",
      "当前代数:84，当前loss为:73.42240905761719\n",
      "当前代数:85，当前loss为:71.09331512451172\n",
      "当前代数:86，当前loss为:68.82415008544922\n",
      "当前代数:87，当前loss为:66.61690521240234\n",
      "当前代数:88，当前loss为:64.46719360351562\n",
      "当前代数:89，当前loss为:62.37372589111328\n",
      "当前代数:90，当前loss为:60.33403778076172\n",
      "当前代数:91，当前loss为:58.345672607421875\n",
      "当前代数:92，当前loss为:56.416378021240234\n",
      "当前代数:93，当前loss为:54.54123306274414\n",
      "当前代数:94，当前loss为:52.7165641784668\n",
      "当前代数:95，当前loss为:50.940975189208984\n",
      "当前代数:96，当前loss为:49.210731506347656\n",
      "当前代数:97，当前loss为:47.527618408203125\n",
      "当前代数:98，当前loss为:45.88788604736328\n",
      "当前代数:99，当前loss为:44.2945671081543\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# fixme: 参数配置\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N为批量大小，D_in是输入维度，H是隐藏层维度，D_out是输出层维度\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "# fixeme: 创建输入和输出随机张量\n",
    "input = torch.randn(N, D_in)\n",
    "label = torch.randn(N, D_out)\n",
    "\n",
    "# fixme: 使用nn包将我们的模型定义为一系列的层\n",
    "## nn.Sequential是包含其他模块的模块，并按顺序应用这些模块来产生其输出。\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "# fixme: nn包还包含常用的损失函数的定义\n",
    "# 在这种情况下，我们将使用平均平方误差(MSE)作为我们的损失函数\n",
    "# 设置reduction='sum'，表示我们计算的是平方误差的“和”，而不是平均值;\n",
    "# 但是在实践中，通过设置reduction='elementwise_mean'来使用均方误差作为损失更为常见。\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# fixme: 使用torch.optim定义参数优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # 这里用的是Adam\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # 前向传播\n",
    "    pred = model(input)\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = loss_fn(pred, label)\n",
    "    print('当前代数:{}，当前loss为:{}'.format(epoch,loss.item()))\n",
    "    \n",
    "    # 反向传播优化器中的模型参数梯度清零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 调用Optimizer的step函数使它所有的参数更新\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea468678-b478-4523-8f9c-61d14a90bcd5",
   "metadata": {},
   "source": [
    "### 自定义nn模块\n",
    "有时候需要制定比现有模块序列更加复杂的模型，对于这些情况，可以通过继承nn.Module并定义forward函数，这个forward函数可以使用其他模块或者其他的自动求导运算来接收输入tensor，产生输出tensor。  \n",
    "\n",
    "下面是一个例子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2ba481f-7e74-4d80-85c0-4816fb766e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前代数:0，当前loss为:661.89208984375\n",
      "当前代数:1，当前loss为:644.700439453125\n",
      "当前代数:2，当前loss为:627.9445190429688\n",
      "当前代数:3，当前loss为:611.715087890625\n",
      "当前代数:4，当前loss为:595.9672241210938\n",
      "当前代数:5，当前loss为:580.7451171875\n",
      "当前代数:6，当前loss为:565.9742431640625\n",
      "当前代数:7，当前loss为:551.5748291015625\n",
      "当前代数:8，当前loss为:537.4945678710938\n",
      "当前代数:9，当前loss为:523.865966796875\n",
      "当前代数:10，当前loss为:510.6148986816406\n",
      "当前代数:11，当前loss为:497.7690734863281\n",
      "当前代数:12，当前loss为:485.2662353515625\n",
      "当前代数:13，当前loss为:473.086669921875\n",
      "当前代数:14，当前loss为:461.2438659667969\n",
      "当前代数:15，当前loss为:449.75933837890625\n",
      "当前代数:16，当前loss为:438.7503662109375\n",
      "当前代数:17，当前loss为:428.12249755859375\n",
      "当前代数:18，当前loss为:417.7576904296875\n",
      "当前代数:19，当前loss为:407.6864318847656\n",
      "当前代数:20，当前loss为:397.8720703125\n",
      "当前代数:21，当前loss为:388.26007080078125\n",
      "当前代数:22，当前loss为:378.8917236328125\n",
      "当前代数:23，当前loss为:369.71429443359375\n",
      "当前代数:24，当前loss为:360.8055419921875\n",
      "当前代数:25，当前loss为:352.11920166015625\n",
      "当前代数:26，当前loss为:343.6255798339844\n",
      "当前代数:27，当前loss为:335.3822937011719\n",
      "当前代数:28，当前loss为:327.343994140625\n",
      "当前代数:29，当前loss为:319.5239562988281\n",
      "当前代数:30，当前loss为:311.87255859375\n",
      "当前代数:31，当前loss为:304.3910827636719\n",
      "当前代数:32，当前loss为:297.0681457519531\n",
      "当前代数:33，当前loss为:289.941162109375\n",
      "当前代数:34，当前loss为:282.9803161621094\n",
      "当前代数:35，当前loss为:276.1708679199219\n",
      "当前代数:36，当前loss为:269.496826171875\n",
      "当前代数:37，当前loss为:262.95367431640625\n",
      "当前代数:38，当前loss为:256.5292663574219\n",
      "当前代数:39，当前loss为:250.24073791503906\n",
      "当前代数:40，当前loss为:244.09030151367188\n",
      "当前代数:41，当前loss为:238.0624542236328\n",
      "当前代数:42，当前loss为:232.1985321044922\n",
      "当前代数:43，当前loss为:226.46063232421875\n",
      "当前代数:44，当前loss为:220.85165405273438\n",
      "当前代数:45，当前loss为:215.3771209716797\n",
      "当前代数:46，当前loss为:210.0019073486328\n",
      "当前代数:47，当前loss为:204.720947265625\n",
      "当前代数:48，当前loss为:199.55430603027344\n",
      "当前代数:49，当前loss为:194.51507568359375\n",
      "当前代数:50，当前loss为:189.56900024414062\n",
      "当前代数:51，当前loss为:184.72769165039062\n",
      "当前代数:52，当前loss为:179.99835205078125\n",
      "当前代数:53，当前loss为:175.36810302734375\n",
      "当前代数:54，当前loss为:170.83486938476562\n",
      "当前代数:55，当前loss为:166.37612915039062\n",
      "当前代数:56，当前loss为:162.01202392578125\n",
      "当前代数:57，当前loss为:157.74380493164062\n",
      "当前代数:58，当前loss为:153.56466674804688\n",
      "当前代数:59，当前loss为:149.47702026367188\n",
      "当前代数:60，当前loss为:145.48004150390625\n",
      "当前代数:61，当前loss为:141.57252502441406\n",
      "当前代数:62，当前loss为:137.7429962158203\n",
      "当前代数:63，当前loss为:133.993896484375\n",
      "当前代数:64，当前loss为:130.32186889648438\n",
      "当前代数:65，当前loss为:126.73446655273438\n",
      "当前代数:66，当前loss为:123.2223129272461\n",
      "当前代数:67，当前loss为:119.78569030761719\n",
      "当前代数:68，当前loss为:116.43122100830078\n",
      "当前代数:69，当前loss为:113.1540756225586\n",
      "当前代数:70，当前loss为:109.94986724853516\n",
      "当前代数:71，当前loss为:106.82859802246094\n",
      "当前代数:72，当前loss为:103.78772735595703\n",
      "当前代数:73，当前loss为:100.81011199951172\n",
      "当前代数:74，当前loss为:97.90054321289062\n",
      "当前代数:75，当前loss为:95.05772399902344\n",
      "当前代数:76，当前loss为:92.28543090820312\n",
      "当前代数:77，当前loss为:89.57870483398438\n",
      "当前代数:78，当前loss为:86.93679809570312\n",
      "当前代数:79，当前loss为:84.35855865478516\n",
      "当前代数:80，当前loss为:81.8379135131836\n",
      "当前代数:81，当前loss为:79.38378143310547\n",
      "当前代数:82，当前loss为:76.99507904052734\n",
      "当前代数:83，当前loss为:74.66697692871094\n",
      "当前代数:84，当前loss为:72.39479064941406\n",
      "当前代数:85，当前loss为:70.18269348144531\n",
      "当前代数:86，当前loss为:68.02684783935547\n",
      "当前代数:87，当前loss为:65.92485046386719\n",
      "当前代数:88，当前loss为:63.87908935546875\n",
      "当前代数:89，当前loss为:61.88774490356445\n",
      "当前代数:90，当前loss为:59.948944091796875\n",
      "当前代数:91，当前loss为:58.058815002441406\n",
      "当前代数:92，当前loss为:56.21525955200195\n",
      "当前代数:93，当前loss为:54.42399978637695\n",
      "当前代数:94，当前loss为:52.6796760559082\n",
      "当前代数:95，当前loss为:50.981727600097656\n",
      "当前代数:96，当前loss为:49.33513641357422\n",
      "当前代数:97，当前loss为:47.726402282714844\n",
      "当前代数:98，当前loss为:46.16450881958008\n",
      "当前代数:99，当前loss为:44.64817810058594\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# fixme: 构建网络\n",
    "class TwoLayerNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        构造函数，在这里需要将网络的各个模块进行实例化, 并把他们作为成员变量\n",
    "        :params D_in: 输入维度\n",
    "        :params H: 隐藏层维度\n",
    "        :params D_out: 输出维度\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(D_in, H)\n",
    "        self.linear2 = nn.Linear(H, D_out)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        构造前向传播函数\n",
    "        \"\"\"\n",
    "        x1 = self.linear1(x)\n",
    "        x_relu = self.relu(x1)\n",
    "        pred = self.linear2(x_relu)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "# fixme: 参数配置\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N为批量大小，D_in是输入维度，H是隐藏层维度，D_out是输出层维度\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "# fixeme: 创建输入和输出随机张量\n",
    "input = torch.randn(N, D_in)\n",
    "label = torch.randn(N, D_out)\n",
    "\n",
    "# fixme: 实例化模型\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "# fixme: 损失函数的定义\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# fixme: 使用torch.optim定义参数优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # 这里用的是Adam\n",
    "\n",
    "# fixme: 训练\n",
    "for epoch in range(epochs):\n",
    "    # 前向过程\n",
    "    pred = model(input)\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = loss_fn(pred, label)\n",
    "    print('当前代数:{}，当前loss为:{}'.format(epoch,loss.item()))\n",
    "    \n",
    "    # 模型参数梯度置零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # loss反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新权重\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022b05e7-28ed-4e94-801a-862c2d86953d",
   "metadata": {},
   "source": [
    "### 控制流和权重共享\n",
    "作为动态图(网络结构发生变化并不影响计算图计算梯度)和权重共享的一个例子，我们实现了一个非常奇怪的模型：一个全连接的ReLU网络，在每一次前向传播时，它的隐藏层的层数为随机1到4之间的数，这样可以多次重用相同的权重来计算。  \n",
    "\n",
    "因为这个模型可以使用普通的Python流控制来实现循环，并且我们可以通过定义转发时多次重用同一个模块来实现最内层的权重共享。\n",
    "\n",
    "下面是例子的代码:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "893b52ce-02a8-40b4-a69a-4fe1d01b52f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "当前代数:0，当前loss为:702.3717651367188\n",
      "当前代数:1，当前loss为:700.1735229492188\n",
      "当前代数:2，当前loss为:697.996826171875\n",
      "当前代数:3，当前loss为:701.37646484375\n",
      "当前代数:4，当前loss为:745.0324096679688\n",
      "当前代数:5，当前loss为:686.2260131835938\n",
      "当前代数:6，当前loss为:729.3468017578125\n",
      "当前代数:7，当前loss为:719.3234252929688\n",
      "当前代数:8，当前loss为:700.3496704101562\n",
      "当前代数:9，当前loss为:697.38720703125\n",
      "当前代数:10，当前loss为:676.840576171875\n",
      "当前代数:11，当前loss为:674.8969116210938\n",
      "当前代数:12，当前loss为:667.5950927734375\n",
      "当前代数:13，当前loss为:657.723388671875\n",
      "当前代数:14，当前loss为:668.5684814453125\n",
      "当前代数:15，当前loss为:691.20361328125\n",
      "当前代数:16，当前loss为:629.1569213867188\n",
      "当前代数:17，当前loss为:662.6659545898438\n",
      "当前代数:18，当前loss为:611.5732421875\n",
      "当前代数:19，当前loss为:602.4924926757812\n",
      "当前代数:20，当前loss为:698.65625\n",
      "当前代数:21，当前loss为:583.9902954101562\n",
      "当前代数:22，当前loss为:655.1369018554688\n",
      "当前代数:23，当前loss为:566.2965087890625\n",
      "当前代数:24，当前loss为:557.3516845703125\n",
      "当前代数:25，当前loss为:547.8927001953125\n",
      "当前代数:26，当前loss为:650.374755859375\n",
      "当前代数:27，当前loss为:648.9633178710938\n",
      "当前代数:28，当前loss为:697.7493286132812\n",
      "当前代数:29，当前loss为:515.4136962890625\n",
      "当前代数:30，当前loss为:507.982421875\n",
      "当前代数:31，当前loss为:500.00030517578125\n",
      "当前代数:32，当前loss为:491.6020812988281\n",
      "当前代数:33，当前loss为:482.8730163574219\n",
      "当前代数:34，当前loss为:697.01220703125\n",
      "当前代数:35，当前loss为:687.0211791992188\n",
      "当前代数:36，当前loss为:638.8866577148438\n",
      "当前代数:37，当前loss为:696.5703735351562\n",
      "当前代数:38，当前loss为:636.4703369140625\n",
      "当前代数:39，当前loss为:685.7159423828125\n",
      "当前代数:40，当前loss为:685.2298583984375\n",
      "当前代数:41，当前loss为:684.5957641601562\n",
      "当前代数:42，当前loss为:695.6959228515625\n",
      "当前代数:43，当前loss为:427.000732421875\n",
      "当前代数:44，当前loss为:682.4891967773438\n",
      "当前代数:45，当前loss为:419.1412048339844\n",
      "当前代数:46，当前loss为:681.0149536132812\n",
      "当前代数:47，当前loss为:694.716552734375\n",
      "当前代数:48，当前loss为:406.95831298828125\n",
      "当前代数:49，当前loss为:678.7310180664062\n",
      "当前代数:50，当前loss为:623.3473510742188\n",
      "当前代数:51，当前loss为:395.0804443359375\n",
      "当前代数:52，当前loss为:693.6585083007812\n",
      "当前代数:53，当前loss为:675.7344360351562\n",
      "当前代数:54，当前loss为:618.6995239257812\n",
      "当前代数:55，当前loss为:616.9414672851562\n",
      "当前代数:56，当前loss为:614.5744018554688\n",
      "当前代数:57，当前loss为:692.4542236328125\n",
      "当前代数:58，当前loss为:609.0692138671875\n",
      "当前代数:59，当前loss为:691.904052734375\n",
      "当前代数:60，当前loss为:603.1943359375\n",
      "当前代数:61，当前loss为:669.9990234375\n",
      "当前代数:62，当前loss为:691.0020141601562\n",
      "当前代数:63，当前loss为:594.264892578125\n",
      "当前代数:64，当前loss为:591.1102294921875\n",
      "当前代数:65，当前loss为:666.8950805664062\n",
      "当前代数:66，当前loss为:665.9771728515625\n",
      "当前代数:67，当前loss为:689.326171875\n",
      "当前代数:68，当前loss为:688.93701171875\n",
      "当前代数:69，当前loss为:688.5073852539062\n",
      "当前代数:70，当前loss为:574.0647583007812\n",
      "当前代数:71，当前loss为:661.115234375\n",
      "当前代数:72，当前loss为:660.0462036132812\n",
      "当前代数:73，当前loss为:566.465576171875\n",
      "当前代数:74，当前loss为:360.7765197753906\n",
      "当前代数:75，当前loss为:685.8043212890625\n",
      "当前代数:76，当前loss为:357.83026123046875\n",
      "当前代数:77，当前loss为:556.7740478515625\n",
      "当前代数:78，当前loss为:684.478515625\n",
      "当前代数:79，当前loss为:683.9954223632812\n",
      "当前代数:80，当前loss为:683.468994140625\n",
      "当前代数:81，当前loss为:650.9133911132812\n",
      "当前代数:82，当前loss为:545.8726806640625\n",
      "当前代数:83，当前loss为:543.443359375\n",
      "当前代数:84，当前loss为:540.5396118164062\n",
      "当前代数:85，当前loss为:342.5977478027344\n",
      "当前代数:86，当前loss为:645.627685546875\n",
      "当前代数:87，当前loss为:644.410888671875\n",
      "当前代数:88，当前loss为:642.9930419921875\n",
      "当前代数:89，当前loss为:641.402099609375\n",
      "当前代数:90，当前loss为:524.3102416992188\n",
      "当前代数:91，当前loss为:521.52880859375\n",
      "当前代数:92，当前loss为:518.2946166992188\n",
      "当前代数:93，当前loss为:676.001220703125\n",
      "当前代数:94，当前loss为:633.2501220703125\n",
      "当前代数:95，当前loss为:674.614501953125\n",
      "当前代数:96，当前loss为:673.8367919921875\n",
      "当前代数:97，当前loss为:628.274169921875\n",
      "当前代数:98，当前loss为:626.4828491210938\n",
      "当前代数:99，当前loss为:325.72052001953125\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "\n",
    "# fixme: 定义网络\n",
    "class DynamicNet(nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        构造函数，在这里需要将网络的各个模块进行实例化, 并把他们作为成员变量\n",
    "        :params D_in: 输入维度\n",
    "        :params H: 隐藏层维度\n",
    "        :params D_out: 输出维度\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        \n",
    "        self.input_layer = nn.Linear(D_in, H)\n",
    "        self.hidden_layer = nn.Linear(H, H)\n",
    "        self.output_layer = nn.Linear(H, D_out)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.input_layer(x)\n",
    "        t_relu = self.relu(x1)\n",
    "        \n",
    "        # 定义0-3个隐藏层，利用pytorch动态图的特征，这种做法是可行的\n",
    "        ## 重复调用self.hidden_layer 0-3次，由于pytorch是采用动态图的，因此每一次forward都会创建一个新的动态图，不影响梯度计算\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            t_relu = self.relu(self.hidden_layer(t_relu))\n",
    "        \n",
    "        pred = self.output_layer(t_relu)\n",
    "        \n",
    "        return pred\n",
    "\n",
    "# fixme: 参数配置\n",
    "dtype = torch.float\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10 # N为批量大小，D_in是输入维度，H是隐藏层维度，D_out是输出层维度\n",
    "learning_rate = 1e-4\n",
    "epochs = 100\n",
    "\n",
    "# fixeme: 创建输入和输出随机张量\n",
    "input = torch.randn(N, D_in)\n",
    "label = torch.randn(N, D_out)\n",
    "\n",
    "# fixme: 实例化模型\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "# fixme: 损失函数的定义\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "# fixme: 使用torch.optim定义参数优化器\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate) # 这里用的是Adam\n",
    "\n",
    "# fixme: 训练\n",
    "for epoch in range(epochs):\n",
    "    # 前向过程\n",
    "    pred = model(input)\n",
    "    \n",
    "    # 计算loss\n",
    "    loss = loss_fn(pred, label)\n",
    "    print('当前代数:{}，当前loss为:{}'.format(epoch,loss.item()))\n",
    "    \n",
    "    # 模型参数梯度置零\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # loss反向传播\n",
    "    loss.backward()\n",
    "    \n",
    "    # 更新权重\n",
    "    optimizer.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
